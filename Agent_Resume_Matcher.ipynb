{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantuonweb/Google_Collab/blob/master/Agent_Resume_Matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4f7a981c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7a981c",
        "outputId": "2568f6c6-12a9-44ba-f1e5-fe21f85dc76e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.6)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Using cached langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.5)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.5.0)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "Using cached langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: langchain-text-splitters, langchain-community\n",
            "Successfully installed langchain-community-0.4.1 langchain-text-splitters-1.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-openai langchain-community langchain-text-splitters langchain-core faiss-cpu python-dotenv pypdf langchain-openai langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "faf9e212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf9e212",
        "outputId": "ff54b6ef-e551-46a3-dd19-62ae5c158f11",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Setup complete\n",
            "API Key: sk-proj-2D_k1B8OV3MW...\n",
            "‚úì Folders created: ./resumes and ./resume_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "load_dotenv()\n",
        "print(\"‚úì Setup complete\")\n",
        "print(\"API Key:\", os.environ.get(\"OPENAI_API_KEY\", \"Not set\")[:20] + \"...\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(\"./resumes\", exist_ok=True)\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Folders created: ./resumes and ./resume_db\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eb34f5"
      },
      "source": [
        "### 1. Define your functions as LangChain Tools\n",
        "\n",
        "LangChain's `@tool` decorator allows you to expose Python functions to an LLM agent. The docstring of the function is crucial as the agent uses it to understand what the tool does and what arguments it expects."
      ],
      "id": "a0eb34f5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "112f54c3"
      },
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "# Wrap the existing functions as tools\n",
        "\n",
        "@tool\n",
        "def ingest_resumes_tool():\n",
        "    \"\"\"Ingest new resumes from the './resumes' folder into the vector database. Use this tool when new resumes need to be processed or the database needs to be updated.\"\"\"\n",
        "    return ingest_resumes()\n",
        "\n",
        "@tool\n",
        "def list_resumes_tool():\n",
        "    \"\"\"List all the unique resume file names currently stored in the vector database. Use this tool to see what resumes have been ingested.\"\"\"\n",
        "    return list_resumes()\n",
        "\n",
        "@tool\n",
        "def search_resumes_tool(skills: str):\n",
        "    \"\"\"Search for candidates whose resumes match the given skills. Input should be a comma-separated string of required skills (e.g., 'Python, Machine Learning, Docker'). Use this tool to find candidates for a job opening.\"\"\"\n",
        "    return search_resumes(skills)\n",
        "\n",
        "@tool\n",
        "def clear_resumes_tool():\n",
        "    \"\"\"Clear all resumes from the vector database. This will delete the entire resume database. Use this tool to start fresh or remove all stored resume data.\"\"\"\n",
        "    return clear_resumes()\n",
        "\n",
        "# Note: generate_resume and save_resume are not included as direct agent tools here\n",
        "# because they typically involve structured input (dictionaries) that are harder for a generic agent to construct directly from natural language.\n",
        "# However, you could create a more complex tool that takes simpler inputs and then constructs the dictionary internally.\n",
        "\n",
        "\n",
        "# Create a list of all available tools\n",
        "tools = [\n",
        "    ingest_resumes_tool,\n",
        "    list_resumes_tool,\n",
        "    search_resumes_tool,\n",
        "    clear_resumes_tool\n",
        "]"
      ],
      "id": "112f54c3",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "080becf9"
      },
      "source": [
        "### 2. Set up the LangChain Agent\n",
        "\n",
        "Now, you'll need an LLM to act as the agent's 'brain' and an `AgentExecutor` to run the agent with the defined tools."
      ],
      "id": "080becf9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f79043b7",
        "outputId": "9633621c-bd8a-4f92-e065-ba3d39b9d404"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain.agents import create_agent\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from google.colab import userdata\n",
        "# Initialize embeddings globally\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Define the actual resume management functions\n",
        "\n",
        "def ingest_resumes():\n",
        "    \"\"\"Load resumes from ./resumes folder and add to vector database\"\"\"\n",
        "    print(\"üì• Ingesting resumes...\")\n",
        "    # Load text files\n",
        "    txt_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    txt_docs = txt_loader.load()\n",
        "    # Load PDF files\n",
        "    pdf_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "    all_docs = txt_docs + pdf_docs\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"‚ùå No resumes found in ./resumes folder\")\n",
        "        return \"No resumes found in ./resumes folder\"\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    # Check if FAISS index file exists (not just folder)\n",
        "    db_file_exists = os.path.exists(\"./resume_db/index.faiss\")\n",
        "\n",
        "    if db_file_exists:\n",
        "        # Load existing and add new documents\n",
        "        vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "        vectorstore.add_documents(chunks)\n",
        "        print(f\"‚úì Added {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "        result = f\"Added {len(chunks)} chunks from {len(all_docs)} resumes to existing database\"\n",
        "    else:\n",
        "        # Create new vector store\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        print(f\"‚úì Created new database with {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "        result = f\"Created new database with {len(chunks)} chunks from {len(all_docs)} resumes\"\n",
        "\n",
        "    vectorstore.save_local(\"./resume_db\")\n",
        "    print(\"‚úì Database saved successfully\")\n",
        "    return result\n",
        "\n",
        "def list_resumes():\n",
        "    \"\"\"List all resumes stored in vector database\"\"\"\n",
        "    print(\"üìã Listing resumes...\")\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return \"No database found. Please ingest resumes first.\"\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "    # Get all documents\n",
        "    all_docs = vectorstore.docstore._dict\n",
        "\n",
        "    # Extract unique sources\n",
        "    sources = set()\n",
        "    for doc in all_docs.values():\n",
        "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
        "            sources.add(os.path.basename(doc.metadata['source']))\n",
        "\n",
        "    result = f\"Found {len(sources)} resumes in database:\\n\"\n",
        "    for i, source in enumerate(sorted(sources), 1):\n",
        "        result += f\"{i}. {source}\\n\"\n",
        "        print(f\" {i}. {source}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def search_resumes(skills):\n",
        "    \"\"\"Search resumes by skills and return best matches\"\"\"\n",
        "    print(f\"üîç Searching for candidates with skills: {skills}\")\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return \"No database found. Please ingest resumes first.\"\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Search for relevant resume chunks\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(skills)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    context = \"\\n\\n\".join([f\"Resume {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"You are a recruiter assistant. Based on the following resume excerpts, identify and rank the best candidates for the required skills.\n",
        "\n",
        "Required Skills: {skills}\n",
        "\n",
        "Resume Excerpts:\n",
        "{context}\n",
        "\n",
        "Please provide a quick summary for the top 3 best matching candidates. For each candidate, include their relevant skills, why they are a good fit, and a matching percentage. The response should be concise.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Get LLM response\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ SEARCH RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(response.content)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "def clear_resumes():\n",
        "    \"\"\"Clear all resumes from vector database\"\"\"\n",
        "    print(\"üóëÔ∏è Clearing resume database...\")\n",
        "    if os.path.exists(\"./resume_db\"):\n",
        "        shutil.rmtree(\"./resume_db\")\n",
        "        print(\"‚úì Database cleared successfully\")\n",
        "        return \"Database cleared successfully\"\n",
        "    else:\n",
        "        print(\"‚ùå No database found\")\n",
        "        return \"No database found\"\n",
        "\n",
        "# Now wrap the functions as tools\n",
        "\n",
        "@tool\n",
        "def ingest_resumes_tool():\n",
        "    \"\"\"Ingest new resumes from the './resumes' folder into the vector database. Use this tool when new resumes need to be processed or the database needs to be updated.\"\"\"\n",
        "    return ingest_resumes()\n",
        "\n",
        "@tool\n",
        "def list_resumes_tool():\n",
        "    \"\"\"List all the unique resume file names currently stored in the vector database. Use this tool to see what resumes have been ingested.\"\"\"\n",
        "    return list_resumes()\n",
        "\n",
        "@tool\n",
        "def search_resumes_tool(skills: str):\n",
        "    \"\"\"Search for candidates whose resumes match the given skills. Input should be a comma-separated string of required skills (e.g., 'Python, Machine Learning, Docker'). Use this tool to find candidates for a job opening.\"\"\"\n",
        "    return search_resumes(skills)\n",
        "\n",
        "@tool\n",
        "def clear_resumes_tool():\n",
        "    \"\"\"Clear all resumes from the vector database. This will delete the entire resume database. Use this tool to start fresh or remove all stored resume data.\"\"\"\n",
        "    return clear_resumes()\n",
        "\n",
        "# Create a list of all available tools\n",
        "tools = [\n",
        "    ingest_resumes_tool,\n",
        "    list_resumes_tool,\n",
        "    search_resumes_tool,\n",
        "    clear_resumes_tool\n",
        "]\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Create the agent\n",
        "agent_executor = create_agent(llm, tools)\n",
        "\n",
        "print(\"‚úì Agent and tools set up successfully.\")\n"
      ],
      "id": "f79043b7",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Agent and tools set up successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0466977"
      },
      "source": [
        "### 3. Use the Agent with Natural Language Queries\n",
        "\n",
        "Now you can interact with your agent using natural language, and it will decide which tool (or tools) to use."
      ],
      "id": "d0466977"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume(data):\n",
        "    \"\"\"Generate a text resume from data dictionary\"\"\"\n",
        "    resume = []\n",
        "\n",
        "    # Header\n",
        "    resume.append(data['name'].upper())\n",
        "    resume.append(f\"{data['email']} | {data['phone']} | {data['location']}\")\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Skills\n",
        "    resume.append(\"SKILLS\")\n",
        "    resume.append(\", \".join(data['skills']))\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Experience\n",
        "    resume.append(\"EXPERIENCE\")\n",
        "    for exp in data['experiences']:\n",
        "        resume.append(f\"{exp['title']} | {exp['company']} | {exp['duration']}\")\n",
        "        for resp in exp['responsibilities']:\n",
        "            resume.append(f\"- {resp}\")\n",
        "        resume.append(\"\")\n",
        "\n",
        "    # Education\n",
        "    resume.append(\"EDUCATION\")\n",
        "    edu = data['education']\n",
        "    resume.append(f\"{edu['degree']} | {edu['institution']} | {edu['year']}\")\n",
        "\n",
        "    return \"\\n\".join(resume)\n",
        "\n",
        "\n",
        "def save_resume(data, filepath):\n",
        "    \"\"\"Save resume to file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(generate_resume(data))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resume_data = {\n",
        "        \"name\": \"Mantu Nigam\",\n",
        "        \"email\": \"mantu.nigam@email.com\",\n",
        "        \"phone\": \"+91-9876543210\",\n",
        "        \"location\": \"Bangalore\",\n",
        "        \"skills\": [\"Python\", \"React\", \"Angular\", \"Nest\", \"Html\", \"CSS\", \"Google Cloud\", \"Docker\"],\n",
        "        \"experiences\": [\n",
        "            {\n",
        "                \"title\": \"Senior AI Engineer\",\n",
        "                \"company\": \"TechCorp\",\n",
        "                \"duration\": \"2021-Present\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Built Full stack applications with Angular and Nest\",\n",
        "                    \"Developed Mobile App Using Material UI\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Software Engineer\",\n",
        "                \"company\": \"TCS\",\n",
        "                \"duration\": \"2022-2025\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Created ML models and REST APIs with Python\"\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"education\": {\n",
        "            \"degree\": \"MCA\",\n",
        "            \"institution\": \"IPU Delhi\",\n",
        "            \"year\": \"2011\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate and print\n",
        "    print(generate_resume(resume_data))\n",
        "\n",
        "    # Save to file\n",
        "    save_resume(resume_data, \"resumes/mantu_nigam_resume.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiKRvsmxYG9f",
        "outputId": "905f7988-84ac-4dea-9846-5024cea202fb"
      },
      "id": "SiKRvsmxYG9f",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MANTU NIGAM\n",
            "mantu.nigam@email.com | +91-9876543210 | Bangalore\n",
            "\n",
            "SKILLS\n",
            "Python, React, Angular, Nest, Html, CSS, Google Cloud, Docker\n",
            "\n",
            "EXPERIENCE\n",
            "Senior AI Engineer | TechCorp | 2021-Present\n",
            "- Built Full stack applications with Angular and Nest\n",
            "- Developed Mobile App Using Material UI\n",
            "\n",
            "Software Engineer | TCS | 2022-2025\n",
            "- Created ML models and REST APIs with Python\n",
            "\n",
            "EDUCATION\n",
            "MCA | IPU Delhi | 2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume(data):\n",
        "    \"\"\"Generate a text resume from data dictionary\"\"\"\n",
        "    resume = []\n",
        "\n",
        "    # Header\n",
        "    resume.append(data['name'].upper())\n",
        "    resume.append(f\"{data['email']} | {data['phone']} | {data['location']}\")\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Skills\n",
        "    resume.append(\"SKILLS\")\n",
        "    resume.append(\", \".join(data['skills']))\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Experience\n",
        "    resume.append(\"EXPERIENCE\")\n",
        "    for exp in data['experiences']:\n",
        "        resume.append(f\"{exp['title']} | {exp['company']} | {exp['duration']}\")\n",
        "        for resp in exp['responsibilities']:\n",
        "            resume.append(f\"- {resp}\")\n",
        "        resume.append(\"\")\n",
        "\n",
        "    # Education\n",
        "    resume.append(\"EDUCATION\")\n",
        "    edu = data['education']\n",
        "    resume.append(f\"{edu['degree']} | {edu['institution']} | {edu['year']}\")\n",
        "\n",
        "    return \"\\n\".join(resume)\n",
        "\n",
        "\n",
        "def save_resume(data, filepath):\n",
        "    \"\"\"Save resume to file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(generate_resume(data))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resume_data = {\n",
        "        \"name\": \"Vinod Malik\",\n",
        "        \"email\": \"vinod.malik@email.com\",\n",
        "        \"phone\": \"+91-9876543210\",\n",
        "        \"location\": \"Bangalore\",\n",
        "        \"skills\": [\"Python\", \"LangChain\", \"VectorDB\", \"Google Cloud\", \"Docker\"],\n",
        "        \"experiences\": [\n",
        "            {\n",
        "                \"title\": \"Senior AI Engineer\",\n",
        "                \"company\": \"TechCorp\",\n",
        "                \"duration\": \"2021-Present\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Built Full stack Gen AI App\",\n",
        "                    \"Developed Mobile App Using Material UI\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Software Engineer\",\n",
        "                \"company\": \"TCS\",\n",
        "                \"duration\": \"2022-2025\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Created ML models and REST APIs with Python\"\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"education\": {\n",
        "            \"degree\": \"MCA\",\n",
        "            \"institution\": \"IPU Delhi\",\n",
        "            \"year\": \"2011\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate and print\n",
        "    print(generate_resume(resume_data))\n",
        "\n",
        "    # Save to file\n",
        "    save_resume(resume_data, \"resumes/vinod_malik_resume.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_HBG6JGYOeU",
        "outputId": "09385046-bb2c-4986-aa44-4ea6864f0274"
      },
      "id": "K_HBG6JGYOeU",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VINOD MALIK\n",
            "vinod.malik@email.com | +91-9876543210 | Bangalore\n",
            "\n",
            "SKILLS\n",
            "Python, LangChain, VectorDB, Google Cloud, Docker\n",
            "\n",
            "EXPERIENCE\n",
            "Senior AI Engineer | TechCorp | 2021-Present\n",
            "- Built Full stack Gen AI App\n",
            "- Developed Mobile App Using Material UI\n",
            "\n",
            "Software Engineer | TCS | 2022-2025\n",
            "- Created ML models and REST APIs with Python\n",
            "\n",
            "EDUCATION\n",
            "MCA | IPU Delhi | 2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23d1f0d",
        "outputId": "843355cc-203d-46ea-d694-c30e8d9eaf66"
      },
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Example 1: List existing resumes\n",
        "print(\"\\n--- Agent Query: List resumes ---\")\n",
        "response = agent_executor.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What resumes do I have?\")]\n",
        "})\n",
        "print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 2: Search for candidates\n",
        "print(\"\\n--- Agent Query: Search for candidates with Angular and NestJS ---\")\n",
        "response = agent_executor.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Find candidates who are good in Angular and NestJS.\")]\n",
        "})\n",
        "print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 3: Ingest resumes (if you had new files in ./resumes folder)\n",
        "# Make sure there are new files in the './resumes' directory before running this example\n",
        "# print(\"\\n--- Agent Query: Ingest new resumes ---\")\n",
        "# response = agent_executor.invoke({\n",
        "#     \"messages\": [HumanMessage(content=\"Please process any new resumes.\")]\n",
        "# })\n",
        "# print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 4: Clear the database\n",
        "# print(\"\\n--- Agent Query: Clear all resume data ---\")\n",
        "# response = agent_executor.invoke({\n",
        "#     \"messages\": [HumanMessage(content=\"Delete all stored resumes.\")]\n",
        "# })\n",
        "# print(\"Agent Response:\", response[\"messages\"][-1].content)\n"
      ],
      "id": "e23d1f0d",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Agent Query: List resumes ---\n",
            "üìã Listing resumes...\n",
            " 1. mantu_nigam_resume.txt\n",
            " 2. vinod_malik_resume.txt\n",
            "Agent Response: You have the following resumes in the database:\n",
            "\n",
            "1. mantu_nigam_resume.txt\n",
            "2. vinod_malik_resume.txt\n",
            "\n",
            "--- Agent Query: Search for candidates with Angular and NestJS ---\n",
            "üîç Searching for candidates with skills: Angular, NestJS\n",
            "\n",
            "============================================================\n",
            "üéØ SEARCH RESULTS\n",
            "============================================================\n",
            "### Top Candidates for Required Skills: Angular, NestJS\n",
            "\n",
            "#### 1. Mantu Nigam\n",
            "- **Relevant Skills:** Angular, Nest, Python, React, HTML, CSS, Google Cloud, Docker\n",
            "- **Why They Are a Good Fit:** Mantu has direct experience building full-stack applications using both Angular and Nest, which aligns perfectly with the required skills. His background in AI engineering also adds value to his technical expertise.\n",
            "- **Matching Percentage:** 100%\n",
            "\n",
            "#### 2. Vinod Malik\n",
            "- **Relevant Skills:** Python, LangChain, VectorDB, Google Cloud, Docker\n",
            "- **Why They Are a Good Fit:** While Vinod has strong skills in Python and experience in AI applications, he lacks direct experience with Angular and Nest. His background in AI could be beneficial, but he does not meet the primary skill requirements.\n",
            "- **Matching Percentage:** 0%\n",
            "\n",
            "### Summary\n",
            "- **Mantu Nigam** is the only candidate who meets the required skills of Angular and NestJS, making him the top choice. Vinod Malik, while skilled, does not possess the necessary experience with the required technologies.\n",
            "============================================================\n",
            "Agent Response: The top candidate for the required skills of Angular and NestJS is:\n",
            "\n",
            "#### Mantu Nigam\n",
            "- **Relevant Skills:** Angular, Nest, Python, React, HTML, CSS, Google Cloud, Docker\n",
            "- **Why They Are a Good Fit:** Mantu has direct experience building full-stack applications using both Angular and Nest, which aligns perfectly with the required skills. His background in AI engineering also adds value to his technical expertise.\n",
            "- **Matching Percentage:** 100%\n",
            "\n",
            "#### Other Candidate:\n",
            "- **Vinod Malik**\n",
            "  - **Relevant Skills:** Python, LangChain, VectorDB, Google Cloud, Docker\n",
            "  - **Why They Are a Good Fit:** While Vinod has strong skills in Python and experience in AI applications, he lacks direct experience with Angular and Nest. His background in AI could be beneficial, but he does not meet the primary skill requirements.\n",
            "  - **Matching Percentage:** 0%\n",
            "\n",
            "### Summary\n",
            "Mantu Nigam is the only candidate who meets the required skills of Angular and NestJS, making him the top choice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Example 1: List existing resumes\n",
        "print(\"\\n--- Agent Query: List resumes ---\")\n",
        "response = agent_executor.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What resumes do I have?\")]\n",
        "})\n",
        "print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 2: Search for candidates\n",
        "print(\"\\n--- Agent Query: Search for candidates with python and ML ---\")\n",
        "response = agent_executor.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Find candidates who are good in python and ML.\")]\n",
        "})\n",
        "print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 3: Ingest resumes (if you had new files in ./resumes folder)\n",
        "# Make sure there are new files in the './resumes' directory before running this example\n",
        "# print(\"\\n--- Agent Query: Ingest new resumes ---\")\n",
        "# response = agent_executor.invoke({\n",
        "#     \"messages\": [HumanMessage(content=\"Please process any new resumes.\")]\n",
        "# })\n",
        "# print(\"Agent Response:\", response[\"messages\"][-1].content)\n",
        "\n",
        "# Example 4: Clear the database\n",
        "# print(\"\\n--- Agent Query: Clear all resume data ---\")\n",
        "# response = agent_executor.invoke({\n",
        "#     \"messages\": [HumanMessage(content=\"Delete all stored resumes.\")]\n",
        "# })\n",
        "# print(\"Agent Response:\", response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0WfMG_rYlPR",
        "outputId": "5694a436-0469-4209-aadd-d7a9ede2c59e"
      },
      "id": "T0WfMG_rYlPR",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Agent Query: List resumes ---\n",
            "üìã Listing resumes...\n",
            " 1. mantu_nigam_resume.txt\n",
            " 2. vinod_malik_resume.txt\n",
            "Agent Response: You have the following resumes in the database:\n",
            "\n",
            "1. mantu_nigam_resume.txt\n",
            "2. vinod_malik_resume.txt\n",
            "\n",
            "--- Agent Query: Search for candidates with python and ML ---\n",
            "üîç Searching for candidates with skills: Python, Machine Learning\n",
            "\n",
            "============================================================\n",
            "üéØ SEARCH RESULTS\n",
            "============================================================\n",
            "### Top Candidates Summary\n",
            "\n",
            "**1. Vinod Malik**  \n",
            "- **Relevant Skills:** Python, Machine Learning (ML), Google Cloud, Docker  \n",
            "- **Why a Good Fit:** Vinod has direct experience in creating ML models and REST APIs using Python, which aligns perfectly with the required skills. His role as a Senior AI Engineer indicates a strong background in AI applications.  \n",
            "- **Matching Percentage:** 90%\n",
            "\n",
            "**2. Mantu Nigam**  \n",
            "- **Relevant Skills:** Python, Machine Learning (ML), Google Cloud, Docker  \n",
            "- **Why a Good Fit:** Mantu also has experience in creating ML models and REST APIs with Python. Although his focus is more on full-stack development, his background in AI engineering suggests a solid understanding of machine learning concepts.  \n",
            "- **Matching Percentage:** 85%\n",
            "\n",
            "Both candidates have strong Python skills and relevant experience in machine learning, making them suitable for the role.\n",
            "============================================================\n",
            "Agent Response: I found two strong candidates who are proficient in Python and Machine Learning:\n",
            "\n",
            "**1. Vinod Malik**  \n",
            "- **Relevant Skills:** Python, Machine Learning (ML), Google Cloud, Docker  \n",
            "- **Why a Good Fit:** Vinod has direct experience in creating ML models and REST APIs using Python, which aligns perfectly with the required skills. His role as a Senior AI Engineer indicates a strong background in AI applications.  \n",
            "- **Matching Percentage:** 90%\n",
            "\n",
            "**2. Mantu Nigam**  \n",
            "- **Relevant Skills:** Python, Machine Learning (ML), Google Cloud, Docker  \n",
            "- **Why a Good Fit:** Mantu also has experience in creating ML models and REST APIs with Python. Although his focus is more on full-stack development, his background in AI engineering suggests a solid understanding of machine learning concepts.  \n",
            "- **Matching Percentage:** 85%\n",
            "\n",
            "Both candidates are well-suited for roles requiring expertise in Python and machine learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqMAAK8bW7t3"
      },
      "id": "BqMAAK8bW7t3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Clear Flush DB or fresh Start"
      ],
      "metadata": {
        "id": "z9sRVphUZHN9"
      },
      "id": "z9sRVphUZHN9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef98f7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef98f7e",
        "outputId": "fdc23856-c964-45ab-a17f-df8a567010f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cleaned up old database\n",
            "‚úì Ready for fresh start\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove corrupted database\n",
        "if os.path.exists(\"./resume_db\"):\n",
        "    shutil.rmtree(\"./resume_db\")\n",
        "    print(\"‚úì Cleaned up old database\")\n",
        "\n",
        "# Recreate folder\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Ready for fresh start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7D_Pz4qT-Mc8"
      },
      "id": "7D_Pz4qT-Mc8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}