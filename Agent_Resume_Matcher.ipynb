{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantuonweb/Google_Collab/blob/master/Agent_Resume_Matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4f7a981c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7a981c",
        "outputId": "5a72af44-e126-49b8-b375-b9ea86681c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.2 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.5 langchain-openai-1.1.6 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 pypdf-6.5.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-openai langchain-community langchain-text-splitters langchain-core faiss-cpu python-dotenv pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "faf9e212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf9e212",
        "outputId": "12e22f99-7458-48b6-f8c2-71c1434ab8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Setup complete\n",
            "API Key: sk-proj-2D_k1B8OV3MW...\n",
            "âœ“ Folders created: ./resumes and ./resume_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "load_dotenv()\n",
        "print(\"âœ“ Setup complete\")\n",
        "print(\"API Key:\", os.environ.get(\"OPENAI_API_KEY\", \"Not set\")[:20] + \"...\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(\"./resumes\", exist_ok=True)\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"âœ“ Folders created: ./resumes and ./resume_db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ef98f7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef98f7e",
        "outputId": "f0c55b7c-6bf7-4682-83f6-65aff5a4517e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Cleaned up old database\n",
            "âœ“ Ready for fresh start\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove corrupted database\n",
        "if os.path.exists(\"./resume_db\"):\n",
        "    shutil.rmtree(\"./resume_db\")\n",
        "    print(\"âœ“ Cleaned up old database\")\n",
        "\n",
        "# Recreate folder\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"âœ“ Ready for fresh start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f105eb87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f105eb87",
        "outputId": "b2551480-1817-43e7-c016-7d3ae60ddc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Agent functions loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize embeddings globally\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def ingest_resumes():\n",
        "    \"\"\"Load resumes from ./resumes folder and add to vector database\"\"\"\n",
        "    print(\"ğŸ“¥ Ingesting resumes...\")\n",
        "\n",
        "    # Load text files\n",
        "    txt_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    txt_docs = txt_loader.load()\n",
        "\n",
        "    # Load PDF files\n",
        "    pdf_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "\n",
        "    all_docs = txt_docs + pdf_docs\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"âŒ No resumes found in ./resumes folder\")\n",
        "        return\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    # Check if FAISS index file exists (not just folder)\n",
        "    db_file_exists = os.path.exists(\"./resume_db/index.faiss\")\n",
        "\n",
        "    if db_file_exists:\n",
        "        # Load existing and add new documents\n",
        "        vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "        vectorstore.add_documents(chunks)\n",
        "        print(f\"âœ“ Added {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "    else:\n",
        "        # Create new vector store\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        print(f\"âœ“ Created new database with {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "\n",
        "    vectorstore.save_local(\"./resume_db\")\n",
        "    print(\"âœ“ Database saved successfully\")\n",
        "\n",
        "\n",
        "def list_resumes():\n",
        "    \"\"\"List all resumes stored in vector database\"\"\"\n",
        "    print(\"ğŸ“‹ Listing resumes...\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"âŒ No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Get all documents\n",
        "    all_docs = vectorstore.docstore._dict\n",
        "\n",
        "    # Extract unique sources\n",
        "    sources = set()\n",
        "    for doc in all_docs.values():\n",
        "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
        "            sources.add(os.path.basename(doc.metadata['source']))\n",
        "\n",
        "    print(f\"\\nâœ“ Found {len(sources)} resumes in database:\")\n",
        "    for i, source in enumerate(sorted(sources), 1):\n",
        "        print(f\"  {i}. {source}\")\n",
        "\n",
        "\n",
        "def search_resumes(skills):\n",
        "    \"\"\"Search resumes by skills and return best matches\"\"\"\n",
        "    print(f\"ğŸ” Searching for candidates with skills: {skills}\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"âŒ No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Search for relevant resume chunks\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(skills)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    context = \"\\n\\n\".join([f\"Resume {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"You are a recruiter assistant. Based on the following resume excerpts, identify and rank the best candidates for the required skills.\n",
        "\n",
        "Required Skills: {skills}\n",
        "\n",
        "Resume Excerpts:\n",
        "{context}\n",
        "\n",
        "Please provide:\n",
        "1. Top 3 best matching candidates\n",
        "2. Their relevant skills and experience\n",
        "3. Why they are a good fit\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Get LLM response\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ¯ SEARCH RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(response.content)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def clear_resumes():\n",
        "    \"\"\"Clear all resumes from vector database\"\"\"\n",
        "    print(\"ğŸ—‘ï¸  Clearing resume database...\")\n",
        "\n",
        "    if os.path.exists(\"./resume_db\"):\n",
        "        shutil.rmtree(\"./resume_db\")\n",
        "        print(\"âœ“ Database cleared successfully\")\n",
        "    else:\n",
        "        print(\"âŒ No database found\")\n",
        "\n",
        "print(\"âœ“ Agent functions loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "113105d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113105d0",
        "outputId": "baf6891c-ee76-4b56-fe39-7d89c9717374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Ingesting resumes...\n",
            "âœ“ Created new database with 2 chunks from 2 resumes\n",
            "âœ“ Database saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Add resumes to database\n",
        "ingest_resumes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0fde6dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0fde6dcd",
        "outputId": "6d3c60db-a81c-49f5-f8f2-643442e9d8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Searching for candidates with skills: Python, Machine Learning, LangChain\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ SEARCH RESULTS\n",
            "============================================================\n",
            "Based on the provided resume excerpts and the required skills of Python, Machine Learning, and LangChain, here are the top candidates ranked by their relevance to the required skills:\n",
            "\n",
            "### 1. Mantu Nigam\n",
            "**Relevant Skills:**\n",
            "- Python\n",
            "- LangChain\n",
            "- Machine Learning (implied through experience with ML models)\n",
            "\n",
            "**Experience:**\n",
            "- **Senior AI Engineer at TechCorp (2021-Present)**: Built RAG applications with LangChain and OpenAI, showcasing direct experience with LangChain and Python.\n",
            "- **Software Engineer at DataMinds (2019-2021)**: Created ML models and REST APIs with Python, demonstrating proficiency in Python and Machine Learning.\n",
            "\n",
            "**Why They Are a Good Fit:**\n",
            "Mantu has direct experience with all three required skills. His current role involves using LangChain and Python in a practical setting, and he has a solid background in Machine Learning. This makes him the strongest candidate for the position.\n",
            "\n",
            "---\n",
            "\n",
            "### 2. Vinod Malik\n",
            "**Relevant Skills:**\n",
            "- Python\n",
            "- Machine Learning (implied through experience with TensorFlow)\n",
            "\n",
            "**Experience:**\n",
            "- **Full Stack Developer at WebSolutions (2020-Present)**: While primarily focused on web development, he has experience with Python and AWS.\n",
            "- **Junior Developer at StartupHub (2019-2020)**: Created web interfaces and APIs, but lacks direct experience with Machine Learning and LangChain.\n",
            "\n",
            "**Why They Are a Good Fit:**\n",
            "Vinod has Python skills and some exposure to Machine Learning through TensorFlow. However, he does not have experience with LangChain, which is a critical requirement. He ranks second due to his Python proficiency and potential to learn.\n",
            "\n",
            "---\n",
            "\n",
            "### 3. (No Third Candidate)\n",
            "**Reasoning:**\n",
            "There are only two candidates provided, and the second candidate (Vinod Malik) does not meet all the required skills, particularly the lack of experience with LangChain. Therefore, there is no third candidate to rank.\n",
            "\n",
            "### Summary:\n",
            "1. **Mantu Nigam** - Best fit with all required skills.\n",
            "2. **Vinod Malik** - Good fit with Python and some Machine Learning experience, but lacks LangChain expertise. \n",
            "\n",
            "In conclusion, Mantu Nigam is the top candidate for the position, while Vinod Malik has some relevant skills but does not fully meet the requirements.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided resume excerpts and the required skills of Python, Machine Learning, and LangChain, here are the top candidates ranked by their relevance to the required skills:\\n\\n### 1. Mantu Nigam\\n**Relevant Skills:**\\n- Python\\n- LangChain\\n- Machine Learning (implied through experience with ML models)\\n\\n**Experience:**\\n- **Senior AI Engineer at TechCorp (2021-Present)**: Built RAG applications with LangChain and OpenAI, showcasing direct experience with LangChain and Python.\\n- **Software Engineer at DataMinds (2019-2021)**: Created ML models and REST APIs with Python, demonstrating proficiency in Python and Machine Learning.\\n\\n**Why They Are a Good Fit:**\\nMantu has direct experience with all three required skills. His current role involves using LangChain and Python in a practical setting, and he has a solid background in Machine Learning. This makes him the strongest candidate for the position.\\n\\n---\\n\\n### 2. Vinod Malik\\n**Relevant Skills:**\\n- Python\\n- Machine Learning (implied through experience with TensorFlow)\\n\\n**Experience:**\\n- **Full Stack Developer at WebSolutions (2020-Present)**: While primarily focused on web development, he has experience with Python and AWS.\\n- **Junior Developer at StartupHub (2019-2020)**: Created web interfaces and APIs, but lacks direct experience with Machine Learning and LangChain.\\n\\n**Why They Are a Good Fit:**\\nVinod has Python skills and some exposure to Machine Learning through TensorFlow. However, he does not have experience with LangChain, which is a critical requirement. He ranks second due to his Python proficiency and potential to learn.\\n\\n---\\n\\n### 3. (No Third Candidate)\\n**Reasoning:**\\nThere are only two candidates provided, and the second candidate (Vinod Malik) does not meet all the required skills, particularly the lack of experience with LangChain. Therefore, there is no third candidate to rank.\\n\\n### Summary:\\n1. **Mantu Nigam** - Best fit with all required skills.\\n2. **Vinod Malik** - Good fit with Python and some Machine Learning experience, but lacks LangChain expertise. \\n\\nIn conclusion, Mantu Nigam is the top candidate for the position, while Vinod Malik has some relevant skills but does not fully meet the requirements.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Search for candidates with specific skills\n",
        "skills = \"Python, Machine Learning, LangChain\"  # Change this to your required skills\n",
        "search_resumes(skills)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "84925c24",
      "metadata": {
        "id": "84925c24"
      },
      "outputs": [],
      "source": [
        "# Simple interactive menu\n",
        "def run_agent():\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ“„ RESUME AGENT - MENU\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. Ingest Resumes\")\n",
        "        print(\"2. List Resumes\")\n",
        "        print(\"3. Search by Skills\")\n",
        "        print(\"4. Clear Database\")\n",
        "        print(\"5. Exit\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            ingest_resumes()\n",
        "        elif choice == \"2\":\n",
        "            list_resumes()\n",
        "        elif choice == \"3\":\n",
        "            skills = input(\"Enter required skills (comma-separated): \")\n",
        "            search_resumes(skills)\n",
        "        elif choice == \"4\":\n",
        "            confirm = input(\"Are you sure? (yes/no): \")\n",
        "            if confirm.lower() == \"yes\":\n",
        "                clear_resumes()\n",
        "        elif choice == \"5\":\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"âŒ Invalid choice. Please try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7D_Pz4qT-Mc8"
      },
      "id": "7D_Pz4qT-Mc8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}