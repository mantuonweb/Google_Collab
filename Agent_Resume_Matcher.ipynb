{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantuonweb/Google_Collab/blob/master/Agent_Resume_Matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7a981c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7a981c",
        "outputId": "92f0b136-be8b-4b9b-ef97-2d0474dced23",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.2 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.5 langchain-openai-1.1.6 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 pypdf-6.5.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-openai langchain-community langchain-text-splitters langchain-core faiss-cpu python-dotenv pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf9e212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf9e212",
        "outputId": "7bbca13c-c00b-4638-fcae-e963eea19fb7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Setup complete\n",
            "API Key: sk-proj-2D_k1B8OV3MW...\n",
            "‚úì Folders created: ./resumes and ./resume_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "load_dotenv()\n",
        "print(\"‚úì Setup complete\")\n",
        "print(\"API Key:\", os.environ.get(\"OPENAI_API_KEY\", \"Not set\")[:20] + \"...\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(\"./resumes\", exist_ok=True)\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Folders created: ./resumes and ./resume_db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef98f7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef98f7e",
        "outputId": "fdc23856-c964-45ab-a17f-df8a567010f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cleaned up old database\n",
            "‚úì Ready for fresh start\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove corrupted database\n",
        "if os.path.exists(\"./resume_db\"):\n",
        "    shutil.rmtree(\"./resume_db\")\n",
        "    print(\"‚úì Cleaned up old database\")\n",
        "\n",
        "# Recreate folder\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Ready for fresh start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f105eb87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f105eb87",
        "outputId": "a55602d1-04c6-483c-c1cf-0eb067984b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Agent functions loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize embeddings globally\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def ingest_resumes():\n",
        "    \"\"\"Load resumes from ./resumes folder and add to vector database\"\"\"\n",
        "    print(\"üì• Ingesting resumes...\")\n",
        "\n",
        "    # Load text files\n",
        "    txt_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    txt_docs = txt_loader.load()\n",
        "\n",
        "    # Load PDF files\n",
        "    pdf_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "\n",
        "    all_docs = txt_docs + pdf_docs\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"‚ùå No resumes found in ./resumes folder\")\n",
        "        return\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    # Check if FAISS index file exists (not just folder)\n",
        "    db_file_exists = os.path.exists(\"./resume_db/index.faiss\")\n",
        "\n",
        "    if db_file_exists:\n",
        "        # Load existing and add new documents\n",
        "        vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "        vectorstore.add_documents(chunks)\n",
        "        print(f\"‚úì Added {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "    else:\n",
        "        # Create new vector store\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        print(f\"‚úì Created new database with {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "\n",
        "    vectorstore.save_local(\"./resume_db\")\n",
        "    print(\"‚úì Database saved successfully\")\n",
        "\n",
        "\n",
        "def list_resumes():\n",
        "    \"\"\"List all resumes stored in vector database\"\"\"\n",
        "    print(\"üìã Listing resumes...\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Get all documents\n",
        "    all_docs = vectorstore.docstore._dict\n",
        "\n",
        "    # Extract unique sources\n",
        "    sources = set()\n",
        "    for doc in all_docs.values():\n",
        "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
        "            sources.add(os.path.basename(doc.metadata['source']))\n",
        "\n",
        "    print(f\"\\n‚úì Found {len(sources)} resumes in database:\")\n",
        "    for i, source in enumerate(sorted(sources), 1):\n",
        "        print(f\"  {i}. {source}\")\n",
        "\n",
        "\n",
        "def search_resumes(skills):\n",
        "    \"\"\"Search resumes by skills and return best matches\"\"\"\n",
        "    print(f\"üîç Searching for candidates with skills: {skills}\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Search for relevant resume chunks\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(skills)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    context = \"\\n\\n\".join([f\"Resume {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"You are a recruiter assistant. Based on the following resume excerpts, identify and rank the best candidates for the required skills.\\n\\nRequired Skills: {skills}\\n\\nResume Excerpts:\\n{context}\\n\\nPlease provide a quick summary for the top 3 best matching candidates. For each candidate, include their relevant skills, why they are a good fit, and a matching percentage. The response should be concise.\\n\\nAnswer:\"\"\"\n",
        "\n",
        "    # Get LLM response\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ SEARCH RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(response.content)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def clear_resumes():\n",
        "    \"\"\"Clear all resumes from vector database\"\"\"\n",
        "    print(\"üóëÔ∏è  Clearing resume database...\")\n",
        "\n",
        "    if os.path.exists(\"./resume_db\"):\n",
        "        shutil.rmtree(\"./resume_db\")\n",
        "        print(\"‚úì Database cleared successfully\")\n",
        "    else:\n",
        "        print(\"‚ùå No database found\")\n",
        "\n",
        "print(\"‚úì Agent functions loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume(data):\n",
        "    \"\"\"Generate a text resume from data dictionary\"\"\"\n",
        "    resume = []\n",
        "\n",
        "    # Header\n",
        "    resume.append(data['name'].upper())\n",
        "    resume.append(f\"{data['email']} | {data['phone']} | {data['location']}\")\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Skills\n",
        "    resume.append(\"SKILLS\")\n",
        "    resume.append(\", \".join(data['skills']))\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Experience\n",
        "    resume.append(\"EXPERIENCE\")\n",
        "    for exp in data['experiences']:\n",
        "        resume.append(f\"{exp['title']} | {exp['company']} | {exp['duration']}\")\n",
        "        for resp in exp['responsibilities']:\n",
        "            resume.append(f\"- {resp}\")\n",
        "        resume.append(\"\")\n",
        "\n",
        "    # Education\n",
        "    resume.append(\"EDUCATION\")\n",
        "    edu = data['education']\n",
        "    resume.append(f\"{edu['degree']} | {edu['institution']} | {edu['year']}\")\n",
        "\n",
        "    return \"\\n\".join(resume)\n",
        "\n",
        "\n",
        "def save_resume(data, filepath):\n",
        "    \"\"\"Save resume to file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(generate_resume(data))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resume_data = {\n",
        "        \"name\": \"Mantu Nigam\",\n",
        "        \"email\": \"mantu.nigam@email.com\",\n",
        "        \"phone\": \"+91-9876543210\",\n",
        "        \"location\": \"Bangalore\",\n",
        "        \"skills\": [\"Python\", \"React\", \"Angular\", \"Nest\", \"Html\", \"CSS\", \"Google Cloud\", \"Docker\"],\n",
        "        \"experiences\": [\n",
        "            {\n",
        "                \"title\": \"Senior AI Engineer\",\n",
        "                \"company\": \"TechCorp\",\n",
        "                \"duration\": \"2021-Present\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Built Full stack applications with Angular and Nest\",\n",
        "                    \"Developed Mobile App Using Material UI\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Software Engineer\",\n",
        "                \"company\": \"TCS\",\n",
        "                \"duration\": \"2022-2025\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Created ML models and REST APIs with Python\"\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"education\": {\n",
        "            \"degree\": \"MCA\",\n",
        "            \"institution\": \"IPU Delhi\",\n",
        "            \"year\": \"2011\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate and print\n",
        "    print(generate_resume(resume_data))\n",
        "\n",
        "    # Save to file\n",
        "    save_resume(resume_data, \"resumes/mantu_nigam_resume.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ptppdiksjS8",
        "outputId": "3803b9a2-1bd2-4d5d-8896-ffca2ae402f3"
      },
      "id": "7ptppdiksjS8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MANTU NIGAM\n",
            "mantu.nigam@email.com | +91-9876543210 | Bangalore\n",
            "\n",
            "SKILLS\n",
            "Python, React, Angular, Nest, Html, CSS, Google Cloud, Docker\n",
            "\n",
            "EXPERIENCE\n",
            "Senior AI Engineer | TechCorp | 2021-Present\n",
            "- Built Full stack applications with Angular and Nest\n",
            "- Developed Mobile App Using Material UI\n",
            "\n",
            "Software Engineer | TCS | 2022-2025\n",
            "- Created ML models and REST APIs with Python\n",
            "\n",
            "EDUCATION\n",
            "MCA | IPU Delhi | 2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume(data):\n",
        "    \"\"\"Generate a text resume from data dictionary\"\"\"\n",
        "    resume = []\n",
        "\n",
        "    # Header\n",
        "    resume.append(data['name'].upper())\n",
        "    resume.append(f\"{data['email']} | {data['phone']} | {data['location']}\")\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Skills\n",
        "    resume.append(\"SKILLS\")\n",
        "    resume.append(\", \".join(data['skills']))\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Experience\n",
        "    resume.append(\"EXPERIENCE\")\n",
        "    for exp in data['experiences']:\n",
        "        resume.append(f\"{exp['title']} | {exp['company']} | {exp['duration']}\")\n",
        "        for resp in exp['responsibilities']:\n",
        "            resume.append(f\"- {resp}\")\n",
        "        resume.append(\"\")\n",
        "\n",
        "    # Education\n",
        "    resume.append(\"EDUCATION\")\n",
        "    edu = data['education']\n",
        "    resume.append(f\"{edu['degree']} | {edu['institution']} | {edu['year']}\")\n",
        "\n",
        "    return \"\\n\".join(resume)\n",
        "\n",
        "\n",
        "def save_resume(data, filepath):\n",
        "    \"\"\"Save resume to file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(generate_resume(data))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resume_data = {\n",
        "        \"name\": \"Vinod Malik\",\n",
        "        \"email\": \"vinod.malik@email.com\",\n",
        "        \"phone\": \"+91-9876543210\",\n",
        "        \"location\": \"Bangalore\",\n",
        "        \"skills\": [\"Python\", \"LangChain\", \"VectorDB\", \"Google Cloud\", \"Docker\"],\n",
        "        \"experiences\": [\n",
        "            {\n",
        "                \"title\": \"Senior AI Engineer\",\n",
        "                \"company\": \"TechCorp\",\n",
        "                \"duration\": \"2021-Present\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Built Full stack Gen AI App\",\n",
        "                    \"Developed Mobile App Using Material UI\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Software Engineer\",\n",
        "                \"company\": \"TCS\",\n",
        "                \"duration\": \"2022-2025\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Created ML models and REST APIs with Python\"\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"education\": {\n",
        "            \"degree\": \"MCA\",\n",
        "            \"institution\": \"IPU Delhi\",\n",
        "            \"year\": \"2011\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate and print\n",
        "    print(generate_resume(resume_data))\n",
        "\n",
        "    # Save to file\n",
        "    save_resume(resume_data, \"resumes/vinod_malik_resume.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpW7GOVicTF0",
        "outputId": "a4e3d6c7-ceef-45ae-81b5-cdee5b74aec0"
      },
      "id": "NpW7GOVicTF0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VINOD MALIK\n",
            "vinod.malik@email.com | +91-9876543210 | Bangalore\n",
            "\n",
            "SKILLS\n",
            "Python, LangChain, VectorDB, Google Cloud, Docker\n",
            "\n",
            "EXPERIENCE\n",
            "Senior AI Engineer | TechCorp | 2021-Present\n",
            "- Built Full stack Gen AI App\n",
            "- Developed Mobile App Using Material UI\n",
            "\n",
            "Software Engineer | TCS | 2022-2025\n",
            "- Created ML models and REST APIs with Python\n",
            "\n",
            "EDUCATION\n",
            "MCA | IPU Delhi | 2011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113105d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113105d0",
        "outputId": "966e7a6d-5c60-4b6a-8257-219b9e0081d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Ingesting resumes...\n",
            "‚úì Created new database with 2 chunks from 2 resumes\n",
            "‚úì Database saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Add resumes to database\n",
        "ingest_resumes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fde6dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "0fde6dcd",
        "outputId": "246ce70e-3d28-4910-a7b7-1933fc74b384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching for candidates with skills: front end, angular, react, microservice using nestjs\n",
            "\n",
            "============================================================\n",
            "üéØ SEARCH RESULTS\n",
            "============================================================\n",
            "### Top Candidates Summary\n",
            "\n",
            "**1. Mantu Nigam**  \n",
            "- **Relevant Skills:** React, Angular, Nest, Full stack development  \n",
            "- **Why They Are a Good Fit:** Mantu has direct experience building full stack applications using Angular and Nest, which aligns perfectly with the required skills. His background in both front-end and back-end technologies makes him a strong candidate for roles involving microservices.  \n",
            "- **Matching Percentage:** 90%\n",
            "\n",
            "**2. Vinod Malik**  \n",
            "- **Relevant Skills:** (Limited relevant skills)  \n",
            "- **Why They Are a Good Fit:** While Vinod has experience as a Senior AI Engineer, his resume does not mention any front-end technologies like Angular or React, nor does it indicate experience with microservices using Nest. His skills are more focused on AI and Python, making him less suitable for the required role.  \n",
            "- **Matching Percentage:** 40%\n",
            "\n",
            "### Summary\n",
            "Mantu Nigam is the clear top candidate due to his relevant experience with both Angular and Nest, while Vinod Malik lacks the necessary front-end skills.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Top Candidates Summary\\n\\n**1. Mantu Nigam**  \\n- **Relevant Skills:** React, Angular, Nest, Full stack development  \\n- **Why They Are a Good Fit:** Mantu has direct experience building full stack applications using Angular and Nest, which aligns perfectly with the required skills. His background in both front-end and back-end technologies makes him a strong candidate for roles involving microservices.  \\n- **Matching Percentage:** 90%\\n\\n**2. Vinod Malik**  \\n- **Relevant Skills:** (Limited relevant skills)  \\n- **Why They Are a Good Fit:** While Vinod has experience as a Senior AI Engineer, his resume does not mention any front-end technologies like Angular or React, nor does it indicate experience with microservices using Nest. His skills are more focused on AI and Python, making him less suitable for the required role.  \\n- **Matching Percentage:** 40%\\n\\n### Summary\\nMantu Nigam is the clear top candidate due to his relevant experience with both Angular and Nest, while Vinod Malik lacks the necessary front-end skills.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Search for candidates with specific skills\n",
        "skills = \"front end, angular, react, microservice using nestjs\"  # Change this to your required skills\n",
        "search_resumes(skills)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for candidates with specific skills\n",
        "skills = \"python, ML, Gen AI\"  # Change this to your required skills\n",
        "search_resumes(skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "BvzuhqPfeh4Y",
        "outputId": "3fdddb1f-625e-4eaa-862d-626ccf94a8c4"
      },
      "id": "BvzuhqPfeh4Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching for candidates with skills: python, ML, Gen AI\n",
            "\n",
            "============================================================\n",
            "üéØ SEARCH RESULTS\n",
            "============================================================\n",
            "### Top Candidates Summary\n",
            "\n",
            "**1. Vinod Malik**  \n",
            "- **Relevant Skills:** Python, ML, Gen AI  \n",
            "- **Why a Good Fit:** Vinod has direct experience in building a full-stack Gen AI application and has created ML models using Python. His role as a Senior AI Engineer indicates a strong background in AI technologies.  \n",
            "- **Matching Percentage:** 95%\n",
            "\n",
            "**2. Mantu Nigam**  \n",
            "- **Relevant Skills:** Python, ML  \n",
            "- **Why a Good Fit:** Mantu has experience in creating ML models and REST APIs with Python. However, he lacks specific experience in Gen AI, which slightly lowers his fit compared to Vinod.  \n",
            "- **Matching Percentage:** 85%\n",
            "\n",
            "**3. (No third candidate)**  \n",
            "- **Why No Third Candidate:** Only two candidates provided relevant experience and skills related to the required skills of Python, ML, and Gen AI. \n",
            "\n",
            "### Summary\n",
            "Vinod Malik is the strongest candidate due to his direct experience with Gen AI, followed by Mantu Nigam, who has solid Python and ML experience but lacks Gen AI exposure.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Top Candidates Summary\\n\\n**1. Vinod Malik**  \\n- **Relevant Skills:** Python, ML, Gen AI  \\n- **Why a Good Fit:** Vinod has direct experience in building a full-stack Gen AI application and has created ML models using Python. His role as a Senior AI Engineer indicates a strong background in AI technologies.  \\n- **Matching Percentage:** 95%\\n\\n**2. Mantu Nigam**  \\n- **Relevant Skills:** Python, ML  \\n- **Why a Good Fit:** Mantu has experience in creating ML models and REST APIs with Python. However, he lacks specific experience in Gen AI, which slightly lowers his fit compared to Vinod.  \\n- **Matching Percentage:** 85%\\n\\n**3. (No third candidate)**  \\n- **Why No Third Candidate:** Only two candidates provided relevant experience and skills related to the required skills of Python, ML, and Gen AI. \\n\\n### Summary\\nVinod Malik is the strongest candidate due to his direct experience with Gen AI, followed by Mantu Nigam, who has solid Python and ML experience but lacks Gen AI exposure.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7D_Pz4qT-Mc8"
      },
      "id": "7D_Pz4qT-Mc8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}