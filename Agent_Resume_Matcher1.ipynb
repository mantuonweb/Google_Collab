{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mantuonweb/Google_Collab/blob/master/Agent_Resume_Matcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7a981c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f7a981c",
        "outputId": "ce192026-0630-4b1b-d402-f80e8bfef067",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.6)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.5)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.5.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install langchain-openai langchain-community langchain-text-splitters langchain-core faiss-cpu python-dotenv pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf9e212",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf9e212",
        "outputId": "3014eb0a-43d2-4242-8e16-613c7fbe8bf6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Setup complete\n",
            "API Key: sk-proj-2D_k1B8OV3MW...\n",
            "‚úì Folders created: ./resumes and ./resume_db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "load_dotenv()\n",
        "print(\"‚úì Setup complete\")\n",
        "print(\"API Key:\", os.environ.get(\"OPENAI_API_KEY\", \"Not set\")[:20] + \"...\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(\"./resumes\", exist_ok=True)\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Folders created: ./resumes and ./resume_db\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef98f7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef98f7e",
        "outputId": "d42d6ddb-7f01-496b-df2a-cf9b5db770ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cleaned up old database\n",
            "‚úì Ready for fresh start\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove corrupted database\n",
        "if os.path.exists(\"./resume_db\"):\n",
        "    shutil.rmtree(\"./resume_db\")\n",
        "    print(\"‚úì Cleaned up old database\")\n",
        "\n",
        "# Recreate folder\n",
        "os.makedirs(\"./resume_db\", exist_ok=True)\n",
        "print(\"‚úì Ready for fresh start\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f105eb87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f105eb87",
        "outputId": "549a9f43-7d5e-402e-82df-888081d2cd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Agent functions loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize embeddings globally\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def ingest_resumes():\n",
        "    \"\"\"Load resumes from ./resumes folder and add to vector database\"\"\"\n",
        "    print(\"üì• Ingesting resumes...\")\n",
        "\n",
        "    # Load text files\n",
        "    txt_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    txt_docs = txt_loader.load()\n",
        "\n",
        "    # Load PDF files\n",
        "    pdf_loader = DirectoryLoader(\"./resumes\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "\n",
        "    all_docs = txt_docs + pdf_docs\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"‚ùå No resumes found in ./resumes folder\")\n",
        "        return\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    # Check if FAISS index file exists (not just folder)\n",
        "    db_file_exists = os.path.exists(\"./resume_db/index.faiss\")\n",
        "\n",
        "    if db_file_exists:\n",
        "        # Load existing and add new documents\n",
        "        vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "        vectorstore.add_documents(chunks)\n",
        "        print(f\"‚úì Added {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "    else:\n",
        "        # Create new vector store\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        print(f\"‚úì Created new database with {len(chunks)} chunks from {len(all_docs)} resumes\")\n",
        "\n",
        "    vectorstore.save_local(\"./resume_db\")\n",
        "    print(\"‚úì Database saved successfully\")\n",
        "\n",
        "\n",
        "def list_resumes():\n",
        "    \"\"\"List all resumes stored in vector database\"\"\"\n",
        "    print(\"üìã Listing resumes...\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Get all documents\n",
        "    all_docs = vectorstore.docstore._dict\n",
        "\n",
        "    # Extract unique sources\n",
        "    sources = set()\n",
        "    for doc in all_docs.values():\n",
        "        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n",
        "            sources.add(os.path.basename(doc.metadata['source']))\n",
        "\n",
        "    print(f\"\\n‚úì Found {len(sources)} resumes in database:\")\n",
        "    for i, source in enumerate(sorted(sources), 1):\n",
        "        print(f\"  {i}. {source}\")\n",
        "\n",
        "\n",
        "def search_resumes(skills):\n",
        "    \"\"\"Search resumes by skills and return best matches\"\"\"\n",
        "    print(f\"üîç Searching for candidates with skills: {skills}\")\n",
        "\n",
        "    if not os.path.exists(\"./resume_db/index.faiss\"):\n",
        "        print(\"‚ùå No database found. Please ingest resumes first.\")\n",
        "        return\n",
        "\n",
        "    vectorstore = FAISS.load_local(\"./resume_db\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "    # Search for relevant resume chunks\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    docs = retriever.invoke(skills)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    context = \"\\n\\n\".join([f\"Resume {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
        "\n",
        "    # Create prompt for LLM\n",
        "    prompt = f\"\"\"You are a recruiter assistant. Based on the following resume excerpts, identify and rank the best candidates for the required skills.\\n\\nRequired Skills: {skills}\\n\\nResume Excerpts:\\n{context}\\n\\nPlease provide a quick summary for the top 3 best matching candidates. For each candidate, include their relevant skills, why they are a good fit, and a matching percentage. The response should be concise.\\n\\nAnswer:\"\"\"\n",
        "\n",
        "    # Get LLM response\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéØ SEARCH RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(response.content)\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return response.content\n",
        "\n",
        "\n",
        "def clear_resumes():\n",
        "    \"\"\"Clear all resumes from vector database\"\"\"\n",
        "    print(\"üóëÔ∏è  Clearing resume database...\")\n",
        "\n",
        "    if os.path.exists(\"./resume_db\"):\n",
        "        shutil.rmtree(\"./resume_db\")\n",
        "        print(\"‚úì Database cleared successfully\")\n",
        "    else:\n",
        "        print(\"‚ùå No database found\")\n",
        "\n",
        "print(\"‚úì Agent functions loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resume(data):\n",
        "    \"\"\"Generate a text resume from data dictionary\"\"\"\n",
        "    resume = []\n",
        "\n",
        "    # Header\n",
        "    resume.append(data['name'].upper())\n",
        "    resume.append(f\"{data['email']} | {data['phone']} | {data['location']}\")\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Skills\n",
        "    resume.append(\"SKILLS\")\n",
        "    resume.append(\", \".join(data['skills']))\n",
        "    resume.append(\"\")\n",
        "\n",
        "    # Experience\n",
        "    resume.append(\"EXPERIENCE\")\n",
        "    for exp in data['experiences']:\n",
        "        resume.append(f\"{exp['title']} | {exp['company']} | {exp['duration']}\")\n",
        "        for resp in exp['responsibilities']:\n",
        "            resume.append(f\"- {resp}\")\n",
        "        resume.append(\"\")\n",
        "\n",
        "    # Education\n",
        "    resume.append(\"EDUCATION\")\n",
        "    edu = data['education']\n",
        "    resume.append(f\"{edu['degree']} | {edu['institution']} | {edu['year']}\")\n",
        "\n",
        "    return \"\\n\".join(resume)\n",
        "\n",
        "\n",
        "def save_resume(data, filepath):\n",
        "    \"\"\"Save resume to file\"\"\"\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(generate_resume(data))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resume_data = {\n",
        "        \"name\": \"Mantu Nigam\",\n",
        "        \"email\": \"mantu.nigam@email.com\",\n",
        "        \"phone\": \"+91-9876543210\",\n",
        "        \"location\": \"Bangalore\",\n",
        "        \"skills\": [\"Python\", \"LangChain\", \"OpenAI\", \"RAG\", \"FAISS\", \"FastAPI\", \"AWS\", \"Docker\"],\n",
        "        \"experiences\": [\n",
        "            {\n",
        "                \"title\": \"Senior AI Engineer\",\n",
        "                \"company\": \"TechCorp\",\n",
        "                \"duration\": \"2021-Present\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Built RAG applications with LangChain and OpenAI\",\n",
        "                    \"Developed chatbots using FAISS vector database\"\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"Software Engineer\",\n",
        "                \"company\": \"DataMinds\",\n",
        "                \"duration\": \"2019-2021\",\n",
        "                \"responsibilities\": [\n",
        "                    \"Created ML models and REST APIs with Python\"\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"education\": {\n",
        "            \"degree\": \"B.Tech CS\",\n",
        "            \"institution\": \"IIT Delhi\",\n",
        "            \"year\": \"2019\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Generate and print\n",
        "    print(generate_resume(resume_data))\n",
        "\n",
        "    # Save to file\n",
        "    save_resume(resume_data, \"resumes/mantu_nigam_resume.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ptppdiksjS8",
        "outputId": "98dbeb19-6600-4d5f-aa14-8c9da27f763d"
      },
      "id": "7ptppdiksjS8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MANTU NIGAM\n",
            "mantu.nigam@email.com | +91-9876543210 | Bangalore\n",
            "\n",
            "SKILLS\n",
            "Python, LangChain, OpenAI, RAG, FAISS, FastAPI, AWS, Docker\n",
            "\n",
            "EXPERIENCE\n",
            "Senior AI Engineer | TechCorp | 2021-Present\n",
            "- Built RAG applications with LangChain and OpenAI\n",
            "- Developed chatbots using FAISS vector database\n",
            "\n",
            "Software Engineer | DataMinds | 2019-2021\n",
            "- Created ML models and REST APIs with Python\n",
            "\n",
            "EDUCATION\n",
            "B.Tech CS | IIT Delhi | 2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113105d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "113105d0",
        "outputId": "5b18c9be-bfff-4662-92ff-0996f7845392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Ingesting resumes...\n",
            "‚úì Created new database with 2 chunks from 2 resumes\n",
            "‚úì Database saved successfully\n"
          ]
        }
      ],
      "source": [
        "# Add resumes to database\n",
        "ingest_resumes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fde6dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "0fde6dcd",
        "outputId": "43551686-2127-4ffc-efd9-34bdc2e8468a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching for candidates with skills: front end, angular, react, microservice using nestjs\n",
            "\n",
            "============================================================\n",
            "üéØ SEARCH RESULTS\n",
            "============================================================\n",
            "### Top Candidates Summary\n",
            "\n",
            "**1. Vinod Malik**  \n",
            "- **Relevant Skills:** React, Node.js (Full Stack Development)  \n",
            "- **Why They Are a Good Fit:** Vinod has hands-on experience with React, which is a key requirement. His role as a Full Stack Developer indicates familiarity with front-end technologies and the ability to work on web applications. However, he lacks direct experience with Angular and microservices using NestJS.  \n",
            "- **Matching Percentage:** 70%\n",
            "\n",
            "---\n",
            "\n",
            "**2. Mantu Nigam**  \n",
            "- **Relevant Skills:** None directly related to front-end, Angular, React, or microservices using NestJS  \n",
            "- **Why They Are a Good Fit:** Mantu has strong technical skills in AI and backend development but does not possess any relevant experience in front-end technologies or the specific frameworks required.  \n",
            "- **Matching Percentage:** 10%\n",
            "\n",
            "---\n",
            "\n",
            "**3. (No third candidate)**  \n",
            "- **Relevant Skills:** N/A  \n",
            "- **Why They Are a Good Fit:** There are no additional candidates provided in the excerpts.  \n",
            "- **Matching Percentage:** N/A\n",
            "\n",
            "---\n",
            "\n",
            "### Summary\n",
            "Vinod Malik is the best candidate based on the provided resumes, with relevant experience in React and full-stack development, despite lacking some specific skills. Mantu Nigam does not meet the required skills for this position.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Top Candidates Summary\\n\\n**1. Vinod Malik**  \\n- **Relevant Skills:** React, Node.js (Full Stack Development)  \\n- **Why They Are a Good Fit:** Vinod has hands-on experience with React, which is a key requirement. His role as a Full Stack Developer indicates familiarity with front-end technologies and the ability to work on web applications. However, he lacks direct experience with Angular and microservices using NestJS.  \\n- **Matching Percentage:** 70%\\n\\n---\\n\\n**2. Mantu Nigam**  \\n- **Relevant Skills:** None directly related to front-end, Angular, React, or microservices using NestJS  \\n- **Why They Are a Good Fit:** Mantu has strong technical skills in AI and backend development but does not possess any relevant experience in front-end technologies or the specific frameworks required.  \\n- **Matching Percentage:** 10%\\n\\n---\\n\\n**3. (No third candidate)**  \\n- **Relevant Skills:** N/A  \\n- **Why They Are a Good Fit:** There are no additional candidates provided in the excerpts.  \\n- **Matching Percentage:** N/A\\n\\n---\\n\\n### Summary\\nVinod Malik is the best candidate based on the provided resumes, with relevant experience in React and full-stack development, despite lacking some specific skills. Mantu Nigam does not meet the required skills for this position.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Search for candidates with specific skills\n",
        "skills = \"front end, angular, react, microservice using nestjs\"  # Change this to your required skills\n",
        "search_resumes(skills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84925c24",
      "metadata": {
        "id": "84925c24"
      },
      "outputs": [],
      "source": [
        "# Simple interactive menu\n",
        "def run_agent():\n",
        "    while True:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìÑ RESUME AGENT - MENU\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. Ingest Resumes\")\n",
        "        print(\"2. List Resumes\")\n",
        "        print(\"3. Search by Skills\")\n",
        "        print(\"4. Clear Database\")\n",
        "        print(\"5. Exit\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \")\n",
        "\n",
        "        if choice == \"1\":\n",
        "            ingest_resumes()\n",
        "        elif choice == \"2\":\n",
        "            list_resumes()\n",
        "        elif choice == \"3\":\n",
        "            skills = input(\"Enter required skills (comma-separated): \")\n",
        "            search_resumes(skills)\n",
        "        elif choice == \"4\":\n",
        "            confirm = input(\"Are you sure? (yes/no): \")\n",
        "            if confirm.lower() == \"yes\":\n",
        "                clear_resumes()\n",
        "        elif choice == \"5\":\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Please try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7D_Pz4qT-Mc8"
      },
      "id": "7D_Pz4qT-Mc8",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}